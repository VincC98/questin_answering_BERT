{"cells":[{"cell_type":"markdown","metadata":{"id":"tg7hz_lrYNb9"},"source":["## <span style=\"color:#ff5500\">**Part one:** Data acquisition\n","\n","In this part of the project (Question 3.1 -> 3.5), you will learn how to load and to preprocess a dataset with Huggingface libraries. A Dataset from Huggingface contains the data in a data structure similar to a Python dictionnary.\n","\n","### <span style=\"color:#ff8800\">Data obtention</span>\n","\n","To get an easy access to an open-source dataset, an easy method consists in using the Huggingface library called **datasets**. For this project, we will use the [Stanford Question Answering Dataset (SQuAD)][1], a question answering dataset used to benchmark question answering models. Here is how to load the SQuAD dataset:\n","\n","```python\n","from datasets import load_dataset\n","squad_dataset = load_dataset(\"squad\")\n","```\n","\n","[1]:<https://huggingface.co/datasets/squad>"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sDGXxPSlhQzd"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\vinc\\Documents\\Projects\\LLM\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","squad_dataset = load_dataset(\"squad\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def load_dataset_and_tokenizer():\n","    from datasets import load_dataset\n","    squad_dataset = load_dataset(\"squad\")\n","    import csv\n","    path = \"indices.csv\"#/content/drive/MyDrive/\n","    indices_to_remove_train = []\n","    indices_to_remove_valid = []\n","    with open(path) as f:\n","        reader_obj = csv.reader(f)\n","        indices_to_remove_train = list(map(int, next(reader_obj)))\n","        indices_to_remove_valid = list(map(int, next(reader_obj)))\n","\n","    train_dataset = squad_dataset[\"train\"]\n","    train_dataset = train_dataset.filter(lambda example, idx: idx not in indices_to_remove_train, with_indices=True)\n","\n","    validation_dataset = squad_dataset[\"validation\"]\n","    validation_dataset = validation_dataset.filter(lambda example, idx: idx not in indices_to_remove_valid, with_indices=True)\n","\n","    from transformers import AutoTokenizer\n","\n","    model_checkpoint = \"distilbert-base-cased\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"cLgxNCR6MPrh"},"source":["## Very important\n","SQuAD dataset contains some annotation errors. To solve this issue, below is a filter to remove those elements from both train and validation split. You will need to download this [csv file][1] in order to be able to load the indices to remove (if the link don't work, go to the first task of the project on Inginious). Once downloaded on your local machine, you should upload it to your google drive to have access to it via Colab.\n","\n","[1]:<https://inginious.info.ucl.ac.be/course/LINFO2263/project3a/indices.csv>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MNC0lgN3M8DE"},"outputs":[],"source":["import csv\n","path = \"indices.csv\"#/content/drive/MyDrive/\n","indices_to_remove_train = []\n","indices_to_remove_valid = []\n","with open(path) as f:\n","    reader_obj = csv.reader(f)\n","    indices_to_remove_train = list(map(int, next(reader_obj)))\n","    indices_to_remove_valid = list(map(int, next(reader_obj)))\n","\n","train_dataset = squad_dataset[\"train\"]\n","train_dataset = train_dataset.filter(lambda example, idx: idx not in indices_to_remove_train, with_indices=True)\n","\n","validation_dataset = squad_dataset[\"validation\"]\n","validation_dataset = validation_dataset.filter(lambda example, idx: idx not in indices_to_remove_valid, with_indices=True)"]},{"cell_type":"markdown","metadata":{"id":"_-DXibCLYNb9"},"source":["\n","### <span style=\"color:#ff8800\"> Question 1: Dataset size</span>\n","Here, the squad_dataset is splitted into two splits, the Training split, to train the model, and the Validation split, to evaluate its performance across epochs of training. Both splits contains data that are stored as rows in the dataset. Each row contains several fields (id, title, context, question, answers) that we will called the **features** of the dataset.\n","\n","What are the number of rows of those two splits respectively ? Replace the actual values with those you found.\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["87357\n","10536\n"]}],"source":["\n","raw_size_train = train_dataset.num_rows\n","raw_size_val   = validation_dataset.num_rows\n","\n","print(raw_size_train)\n","print(raw_size_val)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y7V5rxhFn4o8"},"outputs":[{"data":{"text/plain":["{'train': 87357, 'validation': 10536}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["{\"train\":87357, \"validation\":10536}"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{'id': '5733a55a4776f41900660f3b',\n"," 'title': 'University_of_Notre_Dame',\n"," 'context': 'The School of Architecture was established in 1899, although degrees in architecture were first awarded by the university in 1898. Today the school, housed in Bond Hall, offers a five-year undergraduate program leading to the Bachelor of Architecture degree. All undergraduate students study the third year of the program in Rome. The university is globally recognized for its Notre Dame School of Architecture, a faculty that teaches (pre-modernist) traditional and classical architecture and urban planning (e.g. following the principles of New Urbanism and New Classical Architecture). It also awards the renowned annual Driehaus Architecture Prize.',\n"," 'question': 'In what building is the current School of Architecture housed at Notre Dame?',\n"," 'answers': {'text': ['Bond Hall'], 'answer_start': [159]}}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[172]"]},{"cell_type":"markdown","metadata":{"id":"xlWX2FDGYNb_"},"source":["### <span style=\"color:#ff8800\">Question 2: Content</span>\n","\n","What is the value of the feature \"id\" of the row at index 172 of the train split of the squad dataset? What is the value of the feature \"title\" of it?\n","\n","Replace the actual values with those you found\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'5733a55a4776f41900660f3b'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[\"id\"][172]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["'University_of_Notre_Dame'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[\"title\"][172]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"hiOsRLKdoA8Q"},"outputs":[{"data":{"text/plain":["{'id': '5733a55a4776f41900660f3b', 'title': 'University_of_Notre_Dame'}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["{\"id\":'5733a55a4776f41900660f3b', 'title':'University_of_Notre_Dame'}"]},{"cell_type":"markdown","metadata":{"id":"_q8s4DCWYNcA"},"source":["## <span style=\"color:#ff5500\">**Part two**: Data tokenization</span>\n","\n","### <span style=\"color:#ff8800\">Introduction to tokenizers</span>\n","\n","To use a given pretrained transformer model, one needs to load its associated tokenizer in order to format the dataset correctly for the given model and its associated task. In this task, the **!only preprocessing step!** that we will ask you to do is to tokenize the dataset.\n","\n","Before getting into what you will do, let's explain what a [tokenizer from transformers library][8] does. It is a tool that, for example, allows you to:\n","\n","1) Split words string into sub-words token strings and back (Note that a large part of the sub-words token strings will not be splitted but still be considered as sub-words tokens => sub-words token string can be either a splitted or an unsplitted word string)\n","\n","2) Convert sub-words tokens strings into ids (integers) and back\n","\n","3) Add new tokens to the vocabulary (ex: [CLS], [SEP], [PAD], ...)\n","\n","Depending on the parameters given to the tokenizer, it can do a lot of other things. Note that the combination of step 1. and 2. is called encoding/decoding depending on the way in which it is perform. **Split => Convert** is encoding, while **Unconvert => Unsplit** is decoding.\n","\n","\n","### <span style=\"color:#ff8800\">Task specific tokenizer</span>\n","Here, we will use DistilBERT, a distilled version of BERT (in case you want to know more about distillation process: https://en.wikipedia.org/wiki/Knowledge_distillation):\n","\n","```python\n","from transformers import AutoTokenizer\n","\n","model_checkpoint = \"distilbert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","```\n","\n","Now that you have your [tokenizer][6] (a modified version of the [WordPiece tokenizer][7]), in this part, we will ask you to use it to tokenize the Dataset. It will allow you to train and evaluation the Question Answering version of DistilBERT (i.e. the DistilBERT model with the little modifications at the last layers of the model to be able to handle Question Answering task).\n","\n","To do it, you should first use the correct parameters of the tokenizer to, for any question+context pairs present in your dataset, be able to tokenize it this way:\n","\n","**<p align=center>[CLS] question tokens [SEP] context tokens [SEP]</p>**\n","\n","As the tokenizer and the model can only treat a fixed maximum length, sometimes, the context is longer than this length. To handle those cases, we ask you to generate several overlapping questions+contexts pairs from one context.\n","\n","### <span style=\"color:#ff8800\">Tokenizer parameters</span>\n","\n","In this project, the tokenizer can be separated into two parts. Firstly, a truncation part where the question+context pairs are truncated so as not to exceed a given length. Secondly, the [WordPiece][11] part, which is the part that transforms the word tokens present in the truncated question+context pairs into sub-word tokens.\n","\n","For questions 1, 2, 3 and 4 of this part, we ask you to fix several parameters (for the sake of reproducibility and understanding) of the tokenizer to the specific value we give here:\n","\n","- **[stride][1]** (the number of overlapping tokens between two successive chunks): 50\n","- **[max_length][2]** (the maximum size to be treated by the tokenizer): 100\n","- **[padding][3]**: \"max_length\"\n","- **[return_overflowing_tokens][4]**: True\n","\n","Before launching into the following questions, ask yourself this question for each of the parameters you are going to work with for your tokenizer: **Which of the two parts of the tokenizer (Truncation or [WordPiece][11]) concerns this parameter?**\n","\n","While other outputs will be added in the following section by introducing task-specific parameters, here is a description of the outputs of the tokenizer you will use:\n","\n","* ***input_ids***: The list of sub-words token ids to be fed to the model.\n","* ***attention_mask***: A list of 0's and 1's, sub-words tokens being 0's while [PAD] tokens are set to 1. It allows the model to focus its attention only to the useful tokens. Its return is due to **padding** parameter being set.\n","* ***overflowing_to_sample_mapping***: A list of integers working as a map that, for the index of a token sequence returned by the tokenizer, returns the corresponding original sample in the dataset. You will better understand how it works with the example at question 4 of this part.\n","\n","\n",">**Note:** Each name of parameter is clickable with a link to their documentation. It could be useful to understand what each parameter of this tokenizer does.\n","\n",">**Hint:** Try to create a first basic tokenizer with this parameters to see what you can get, and try it on a example of question and context for instance. To be able to decode the tokens, take a look at functions [convert_ids_to_tokens()][10] and [decode()][5] from the tokenizer.\n","\n","[1]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.stride>\n","[2]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.max_length>\n","[3]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.padding>\n","[4]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.padding>\n","[5]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode>\n","[6]:<https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer>\n","[7]:<https://paperswithcode.com/method/wordpiece>\n","[8]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer>\n","[9]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.sequence_ids>\n","[10]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens>\n","[11]:<https://paperswithcode.com/method/wordpiece>"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TTruhqEMlRo5"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"distilbert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#, stride=50, max_length=100, padding=\"max_length\", return_overflowing_tokens=True"]},{"cell_type":"markdown","metadata":{"id":"QStkFz64YNcA"},"source":["### <span style=\"color:#ff8800\">Question 1: Truncation</span>\n","\n","To allow to truncate the dataset with the stride we described, you need to cut the context (not the question) in several parts if too long. To do so, you will use the truncation parameter from your tokenizer, but what type of truncation will you use?\n","Don't forget to add the truncation parameter with its corresponding value to your tokenizer.\n","\n","\n","\n","\n","Give the value it should have to tokenize correctly in the following format:\n","\n",">**Note:** Note that the \"tokenizer\" variable created in the previous cell is a callable object that requires several parameters. Therefore, you should directly call tokenizer(p1, p2, p3, ...) with all the parameters you need to set.\n","\n",">**Hint**: Don't hesitate to test your tokenizer with an example of question and context. When doing it, you will be able to understand what this parameter really does. Also, take a look at the documentation of the [\\_\\_call\\_\\_() function][1] of the tokenizer to determine which parameter you should use.\n","\n","\n","\n","[1]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__>"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"58t0oWfmYNcB"},"outputs":[{"data":{"text/plain":["{'truncation': 'only_second'}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["{\"truncation\": \"only_second\"}"]},{"cell_type":"markdown","metadata":{"id":"YQ3jKGtk77Dw"},"source":["Remember the question you should ask yourself (here it is quite obvious): **Which of the two parts of the tokenizer (Truncation or WordPiece) concerns this parameter?**"]},{"cell_type":"markdown","metadata":{"id":"7R1x8tDLYNcC"},"source":["###  <span style=\"color:#ff8800\">Question 2: Mapping</span>\n","\n","The dataset contains the start index (the first character) of the answer to the question **in the context**. However, when you perform a tokenization, the length of the context will increase, resulting in a different size for the tokenized context. This is due to the fragmentation of some word tokens into subword tokens by [the tokenizer of the version of DistilBERT we use][2], a modified version of [WordPiece][3].\n","\n","Because of this fragmentation, finding the sub-word token that is the start of the answer and the sub-word token that is the end of the answer is hard. In fact, the index of the start of the response is that of the first character of the response starting from the start of the context. The problem is that this index is in the original dataset, which has not yet been tokenized. To find the correspondence between the indices and the sub-word tokens generated by the WordPiece part of the tokenizer, it could be useful to have a mapping between the start and end characters in the original dataset of each sub-word token in the new tokenized dataset. For a given sentence: \"This sub-word is a token\", if its tokenized equivalent is: \"This\" \"sub\" \"##-\" \"##word\" \"is\" \"a\" \"token\", then the mapping would be: [(0,3),(5,7),(8,8),(9,12),(14,15),(17,17),(19,23)]. A parameter of the tokenizer allow to do this mapping during the tokenization process, what is this parameter?\n","\n","Don't forget to add this parameter to your tokenizer.\n","\n","Give the name of the parameter in the following format:\n","\n",">**Hint**: Don't hesitate to test your tokenizer with an example of question and context. When doing it, you will be able to understand what this parameter really does. Also, take a look at the documentation of the [\\_\\_call\\_\\_() function][1] of the tokenizer to determine which parameter you should use.\n","\n","\n","\n","[1]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__>\n","[2]:<https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer>\n","[3]:<https://paperswithcode.com/method/wordpiece>"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"x0KS2usfYNcC"},"outputs":[{"data":{"text/plain":["'return_offsets_mapping '"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["\"return_offsets_mapping \""]},{"cell_type":"markdown","metadata":{"id":"D8Xii_Jx9U5b"},"source":["Remember the question you should ask yourself: **Which of the two parts of the tokenizer (Truncation or WordPiece) concerns this parameter?**"]},{"cell_type":"markdown","metadata":{"id":"GBEGLq_AYNcD"},"source":["### <span style=\"color:#ff8800\">Question 3: Keys of tokenized dataset</span>\n","\n","When you perform tokenization with a tokenizer from Huggingface, the format of the output of the tokenizer is a Python dictionnary. What are the keys of this dictionnary returned when you perform the tokenization on the dataset (you can have it by tokenizing only one example)?\n","\n","As shown below, the format of your answer should be a set containing the name of all keys. Order of elements does not matter.\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[0][\"context\"]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lou', '##rdes', 'France', '?', 'Architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 'V', '##eni', '##te', 'Ad', 'Me', 'O', '##m', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Immediately', 'behind', 'the', 'b', '##asi', '##lica', 'is', 'the']\n","{'input_ids': [[101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","['What', 'is', 'in', 'front', 'of', 'the', 'Notre', 'Dame', 'Main', 'Building', '?', 'Architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 'V', '##eni', '##te', 'Ad', 'Me', 'O', '##m', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Immediately', 'behind', 'the', 'b', '##asi', '##lica', 'is', 'the', 'G', '##rot', '##to', ',']\n","{'input_ids': [[101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","['The', 'Basilica', 'of', 'the', 'Sacred', 'heart', 'at', 'Notre', 'Dame', 'is', 'beside', 'to', 'which', 'structure', '?', 'Architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 'V', '##eni', '##te', 'Ad', 'Me', 'O', '##m', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Immediately', 'behind', 'the', 'b', '##asi', '##lica', 'is', 'the']\n","{'input_ids': [[101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","['What', 'is', 'the', 'G', '##rot', '##to', 'at', 'Notre', 'Dame', '?', 'Architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 'V', '##eni', '##te', 'Ad', 'Me', 'O', '##m', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Immediately', 'behind', 'the', 'b', '##asi', '##lica', 'is', 'the', 'G', '##rot', '##to', ',', 'a']\n","{'input_ids': [[101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","['What', 'sits', 'on', 'top', 'of', 'the', 'Main', 'Building', 'at', 'Notre', 'Dame', '?', 'Architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'At', '##op', 'the', 'Main', 'Building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary', '.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 'V', '##eni', '##te', 'Ad', 'Me', 'O', '##m', '##nes', '\"', '.', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart', '.', 'Immediately', 'behind', 'the', 'b', '##asi', '##lica', 'is', 'the', 'G', '##rot', '##to']\n","{'input_ids': [[101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n"]}],"source":["for i in range(5):\n","    seq1 = train_dataset[i][\"question\"]\n","    #print(type(seq1), seq1)\n","    seq2 = train_dataset[i][\"context\"]\n","    #print(type(seq2), seq2)\n","    print(tokenizer.tokenize(seq1, seq2,stride=50, max_length=100, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\"))\n","    test = tokenizer(seq1, seq2,stride=50, max_length=100, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True)\n","    print(test)\n","    print(test.keys())\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"f8kVv8bYYNcD"},"outputs":[{"data":{"text/plain":["{'attention_mask', 'input_ids', 'offset_mapping', 'overflow_to_sample_mapping'}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["{'input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'}"]},{"cell_type":"markdown","metadata":{"id":"dkK9fVz3YNcD"},"source":["### <span style=\"color:#ff8800\">Question 4: Example</span>\n","\n","With a tokenizer, you can tokenize more than one question+context at a time. To do so, you need to provide a list of question and a list of context to it.\n","Tokenize the 6 first questions+contexts of the train split of the squad dataset. Due to the effects of the parameters we discussed earlier that concerns the truncation part of the tokenizer, several tokenized questions+contexts pairs can appear for a unique original question+context pair.\n","\n","Provide a list that maps each tokenized question+context to its corresponding original question+context. This list should have a length equal to the number of tokenized questions+contexts, and each value of it should be either 0, 1, 2, 3, 4 or 5 based on his mapping to the original questions+contexts.\n","\n","\n","**Example:** You tokenize 2 questions+contexts, and you get 5 tokenized questions+contexts. The provided list could be of this form: [0, 0, 0, 1, 1] if the 3 first tokenized questions+contexts correspond to the first non-tokenized question+context and the 2 last tokenized questions+contexts correspond to the second non-tokenized question+context.\n","\n","Your answer should be a list of integers (Order of elements matters here).\n",">**Hint**: To get this mapping, look at the parameters you use in your tokenizer **that have not been discussed yet in question 1 and 2** and above all at the outputs returned by the tokenization, it could be very useful.\n","\n",">**Hint 2**: This mapping clearly concerns the Truncation part of the tokenizer. What parameters and outputs concern the Truncation part ?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'What is in front of the Notre Dame Main Building?', 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'What is the Grotto at Notre Dame?', 'What sits on top of the Main Building at Notre Dame?', 'When did the Scholastic Magazine of Notre dame begin publishing?']\n","6\n"]}],"source":["six_questions = []\n","six_contexts = []\n","for i in range(0,6):\n","    current_question = train_dataset[i][\"question\"]\n","    current_context = train_dataset[i][\"context\"]\n","    six_questions.append(current_question)\n","    six_contexts.append(current_context)\n","print(six_questions)\n","print(len(six_contexts))"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1249, 1120, 1211, 1168, 5659, 117, 10360, 8022, 112, 188, 1651, 1576, 170, 1295, 1104, 2371, 2394, 11994, 119, 1109, 2551, 2377, 118, 1576, 11994, 1511, 1210, 6195, 117, 1241, 170, 2070, 1105, 1778, 1466, 117, 1105, 1317, 6959, 1105, 9442, 119, 4108, 11652, 1112, 170, 1141, 118, 3674, 4897, 1107, 1347, 6789, 117, 1103, 20452, 14084, 25066, 2435, 1110, 3010, 3059, 7868, 1105, 3711, 1106, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1778, 1466, 117, 1105, 1317, 6959, 1105, 9442, 119, 4108, 11652, 1112, 170, 1141, 118, 3674, 4897, 1107, 1347, 6789, 117, 1103, 20452, 14084, 25066, 2435, 1110, 3010, 3059, 7868, 1105, 3711, 1106, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 25186, 1197, 117, 1110, 1308, 3059, 170, 1214, 1105, 7203, 1113, 2377, 3783, 1105, 8262, 119, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 25186, 1197, 117, 1110, 1308, 3059, 170, 1214, 1105, 7203, 1113, 2377, 3783, 1105, 8262, 119, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 16460, 1502, 3828, 1105, 2871, 7516, 2755, 1105, 1168, 2371, 117, 1105, 23511, 1118, 1651, 1121, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 16460, 1502, 3828, 1105, 2871, 7516, 2755, 1105, 1168, 2371, 117, 1105, 23511, 1118, 1651, 1121, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 117, 1109, 16460, 1110, 1126, 2457, 4128, 1105, 1674, 1136, 1138, 170, 5449, 10292, 1137, 1251, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 117, 1109, 16460, 1110, 1126, 2457, 4128, 1105, 1674, 1136, 1138, 170, 5449, 10292, 1137, 1251, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 1106, 1437, 170, 6588, 15069, 117, 170, 7691, 3054, 117, 6869, 26456, 1108, 1502, 119, 18872, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 1106, 1437, 170, 6588, 15069, 117, 170, 7691, 3054, 117, 6869, 26456, 1108, 1502, 119, 18872, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 6588, 2526, 2600, 16823, 1355, 1154, 1707, 119, 8853, 2526, 1110, 1502, 1112, 1510, 1112, 1109, 16460, 132, 1649, 117, 1155, 1210, 1132, 4901, 1106, 1155, 1651, 119, 4428, 117, 1107, 5350, 1369, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 6588, 2526, 2600, 16823, 1355, 1154, 1707, 119, 8853, 2526, 1110, 1502, 1112, 1510, 1112, 1109, 16460, 132, 1649, 117, 1155, 1210, 1132, 4901, 1106, 1155, 1651, 119, 4428, 117, 1107, 5350, 1369, 1126, 8448, 4897, 1111, 1741, 2598, 1844, 117, 8270, 11207, 117, 1189, 1157, 1963, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (0, 2), (3, 5), (6, 10), (11, 16), (17, 29), (29, 30), (31, 36), (37, 41), (41, 42), (42, 43), (44, 52), (53, 56), (57, 58), (59, 65), (66, 68), (69, 73), (74, 79), (80, 87), (87, 88), (89, 92), (93, 97), (98, 105), (105, 106), (106, 109), (110, 117), (118, 125), (126, 131), (132, 142), (142, 143), (144, 148), (149, 150), (151, 156), (157, 160), (161, 171), (172, 179), (179, 180), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 219), (219, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 270), (270, 273), (273, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (161, 171), (172, 179), (179, 180), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 219), (219, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 270), (270, 273), (273, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (422, 426), (426, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (422, 426), (426, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (1259, 1262), (1263, 1274), (1275, 1277), (1278, 1281), (1282, 1290), (1290, 1291), (1292, 1299), (1299, 1300), (1301, 1303), (1304, 1310), (1311, 1315), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (1259, 1262), (1263, 1274), (1275, 1277), (1278, 1281), (1282, 1290), (1290, 1291), (1292, 1299), (1299, 1300), (1301, 1303), (1304, 1310), (1311, 1315), (1316, 1318), (1319, 1332), (1333, 1340), (1341, 1344), (1345, 1354), (1355, 1362), (1363, 1371), (1371, 1372), (1373, 1379), (1380, 1388), (1388, 1389), (1390, 1394), (1395, 1398), (1399, 1404), (1404, 1405), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5]}\n","<class 'transformers.tokenization_utils_base.BatchEncoding'>\n","dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","input_ids [[101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1109, 19349, 1104, 1103, 11373, 1762, 1120, 10360, 8022, 1110, 3148, 1106, 1134, 2401, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 102], [101, 1327, 1110, 1103, 144, 10595, 2430, 1120, 10360, 8022, 136, 102, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 102], [101, 1327, 7250, 1113, 1499, 1104, 1103, 4304, 4334, 1120, 10360, 8022, 136, 102, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1249, 1120, 1211, 1168, 5659, 117, 10360, 8022, 112, 188, 1651, 1576, 170, 1295, 1104, 2371, 2394, 11994, 119, 1109, 2551, 2377, 118, 1576, 11994, 1511, 1210, 6195, 117, 1241, 170, 2070, 1105, 1778, 1466, 117, 1105, 1317, 6959, 1105, 9442, 119, 4108, 11652, 1112, 170, 1141, 118, 3674, 4897, 1107, 1347, 6789, 117, 1103, 20452, 14084, 25066, 2435, 1110, 3010, 3059, 7868, 1105, 3711, 1106, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1778, 1466, 117, 1105, 1317, 6959, 1105, 9442, 119, 4108, 11652, 1112, 170, 1141, 118, 3674, 4897, 1107, 1347, 6789, 117, 1103, 20452, 14084, 25066, 2435, 1110, 3010, 3059, 7868, 1105, 3711, 1106, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 25186, 1197, 117, 1110, 1308, 3059, 170, 1214, 1105, 7203, 1113, 2377, 3783, 1105, 8262, 119, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1129, 1103, 3778, 6803, 14532, 4128, 1107, 1103, 1244, 1311, 119, 1109, 1168, 2435, 117, 1109, 23915, 25186, 1197, 117, 1110, 1308, 3059, 170, 1214, 1105, 7203, 1113, 2377, 3783, 1105, 8262, 119, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 16460, 1502, 3828, 1105, 2871, 7516, 2755, 1105, 1168, 2371, 117, 1105, 23511, 1118, 1651, 1121, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1109, 17917, 1214, 6470, 1110, 1502, 6089, 119, 1109, 6195, 1138, 9507, 4128, 4740, 117, 1114, 1109, 16460, 1502, 3828, 1105, 2871, 7516, 2755, 1105, 1168, 2371, 117, 1105, 23511, 1118, 1651, 1121, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 117, 1109, 16460, 1110, 1126, 2457, 4128, 1105, 1674, 1136, 1138, 170, 5449, 10292, 1137, 1251, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 1241, 10360, 8022, 1105, 2216, 2090, 112, 188, 1531, 119, 5472, 20452, 14084, 25066, 1105, 1109, 17917, 117, 1109, 16460, 1110, 1126, 2457, 4128, 1105, 1674, 1136, 1138, 170, 5449, 10292, 1137, 1251, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 1106, 1437, 170, 6588, 15069, 117, 170, 7691, 3054, 117, 6869, 26456, 1108, 1502, 119, 18872, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 9378, 20051, 1121, 1103, 1239, 119, 1130, 2164, 117, 1165, 1199, 1651, 2475, 1115, 1109, 16460, 1310, 1106, 1437, 170, 6588, 15069, 117, 170, 7691, 3054, 117, 6869, 26456, 1108, 1502, 119, 18872, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 6588, 2526, 2600, 16823, 1355, 1154, 1707, 119, 8853, 2526, 1110, 1502, 1112, 1510, 1112, 1109, 16460, 132, 1649, 117, 1155, 1210, 1132, 4901, 1106, 1155, 1651, 119, 4428, 117, 1107, 5350, 1369, 102], [101, 1332, 1225, 1103, 20452, 14084, 25066, 4341, 1104, 10360, 6961, 1162, 3295, 5550, 136, 102, 117, 1107, 1581, 117, 1165, 1168, 1651, 2475, 1115, 1103, 2526, 2799, 170, 7691, 15069, 117, 1103, 6588, 2526, 2600, 16823, 1355, 1154, 1707, 119, 8853, 2526, 1110, 1502, 1112, 1510, 1112, 1109, 16460, 132, 1649, 117, 1155, 1210, 1132, 4901, 1106, 1155, 1651, 119, 4428, 117, 1107, 5350, 1369, 1126, 8448, 4897, 1111, 1741, 2598, 1844, 117, 8270, 11207, 117, 1189, 1157, 1963, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","offset_mapping [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 13), (13, 16), (16, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (0, 2), (3, 5), (6, 10), (11, 16), (17, 29), (29, 30), (31, 36), (37, 41), (41, 42), (42, 43), (44, 52), (53, 56), (57, 58), (59, 65), (66, 68), (69, 73), (74, 79), (80, 87), (87, 88), (89, 92), (93, 97), (98, 105), (105, 106), (106, 109), (110, 117), (118, 125), (126, 131), (132, 142), (142, 143), (144, 148), (149, 150), (151, 156), (157, 160), (161, 171), (172, 179), (179, 180), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 219), (219, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 270), (270, 273), (273, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (161, 171), (172, 179), (179, 180), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 219), (219, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 270), (270, 273), (273, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (422, 426), (426, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 422), (422, 426), (426, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (501, 504), (505, 509), (510, 514), (514, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 755), (755, 758), (758, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (1259, 1262), (1263, 1274), (1275, 1277), (1278, 1281), (1282, 1290), (1290, 1291), (1292, 1299), (1299, 1300), (1301, 1303), (1304, 1310), (1311, 1315), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 15), (15, 18), (18, 23), (24, 32), (33, 35), (36, 41), (42, 45), (45, 46), (47, 52), (53, 63), (63, 64), (0, 0), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (1259, 1262), (1263, 1274), (1275, 1277), (1278, 1281), (1282, 1290), (1290, 1291), (1292, 1299), (1299, 1300), (1301, 1303), (1304, 1310), (1311, 1315), (1316, 1318), (1319, 1332), (1333, 1340), (1341, 1344), (1345, 1354), (1355, 1362), (1363, 1371), (1371, 1372), (1373, 1379), (1380, 1388), (1388, 1389), (1390, 1394), (1395, 1398), (1399, 1404), (1404, 1405), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]]\n","overflow_to_sample_mapping [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5]\n"]}],"source":["\n","test = tokenizer(six_questions, six_contexts,stride=50, max_length=100, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True)\n","print(test)\n","print(type(test))\n","print(test.keys())\n","for k,v in test.items():\n","    print(k,v)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"o_LRp0fvq0kh"},"outputs":[{"data":{"text/plain":["[0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 3,\n"," 3,\n"," 3,\n"," 3,\n"," 4,\n"," 4,\n"," 4,\n"," 4,\n"," 5,\n"," 5,\n"," 5,\n"," 5,\n"," 5,\n"," 5,\n"," 5]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5]"]},{"cell_type":"markdown","metadata":{"id":"CIBU8aEdYNcD"},"source":["### <span style=\"color:#ff8800\">Question 5: Tokenized dataset size</span>\n","\n","In the previous questions, you have worked with examples to understand all the parameters used in the tokenizer for this problem. Now, you should be able to tokenize the whole dataset.\n","Once you have tokenized both Training split and Validation split, both must have increased in terms of size, as we already described in previous exercises. How much did this impact the datasets?\n","\n","\n","For both Training set and Validation set, compute the difference in the number of question+context (i.e. their length) between raw dataset and tokenized one. Your answer should be the result of **new_size - raw_size** for each dataset. For this question and until the end of the project, we will ask you to change the values of some parameters of the tokenizer to test it with a more optimized set of parameters than the one used in previous questions for understanding purposes:\n","\n","* **max_length**: 384\n","* **stride**: 128\n","\n","Please follow the format below for your answer. The integers must correspond to computed difference instead of 200 here."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"NcUdpRt2YNcE"},"outputs":[{"data":{"text/plain":["{'training': 200, 'validation': 200}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["{'training':200, 'validation':200}"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["train_tokenized = tokenizer(train_dataset[\"question\"], train_dataset[\"context\"],stride=128, max_length=384, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True)\n","validation_tokenized = tokenizer(validation_dataset[\"question\"], validation_dataset[\"context\"],stride=128, max_length=384, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'transformers.tokenization_utils_base.BatchEncoding'>\n","88463\n"]}],"source":["print(type(train_tokenized))\n","print(len(train_tokenized[\"input_ids\"]))\n","len_tokenized_train = len(train_tokenized[\"input_ids\"])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]}],"source":["print(len(train_tokenized))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["len_tokenized_val = len(validation_tokenized[\"input_ids\"])"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["{'training': 1106, 'validation': 248}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["{'training':(len_tokenized_train - raw_size_train), 'validation':(len_tokenized_val - raw_size_val)}"]},{"cell_type":"markdown","metadata":{"id":"qc3eUJAVYNcE"},"source":["## <span style=\"color:#ff5500\">**Part three**: data labeling</span>\n","\n","The tokenization itself does not provide a useable dataset for the model. The labels still need to be generated.\n","\n","In general, a label is the element we try to predict using a Machine Learning model. It can take many forms depending on the applications. In simple words, it is the thing we're predicting (the y variable in simple linear regression).\n","\n","In the context of Question Answering task, a **label** is a **pair of integer values** defined as follow with start_position and end_position being the **indices of the !!sub-word tokens!!** at the start of the answer and where the answer ends in the tokenized dataset:\n","\n","* **(0,0)**: if the answer is not or partially not in the part of the context being considered (remember that when some of the contexts are truncated, therefore, the answer can sometimes be truncated too).\n","* **(start_position, end_position)**: if the answer is fully in the part of the context being considered\n","\n",">**Note**: The smallest position possible here is (0,0) and it is occupied by the [CLS] token, meaning that the only answers that should be predicted as (0,0) should be the one not or partially in the part of context being considered. Also, note that sometimes, start_position is equal to end_position, when the answer is composed of only one token, such as in the example below.\n","\n","________\n","**Here is an example:**\n","\n","* The question is \"*Which year did the USSR cancel the N1 rocket program after two failures that didn't launch?*\"\n","* The context is \"*Meanwhile, the USSR continued briefly trying to perfect their N1 rocket, finally canceling it in 1976, after two more launch failures in 1971 and 1972.*\"\n","* The answer is :\n","    * Text: \"*1976*\"\n","    * Answer_start: \"*97*\"\n","* The tokenized question+context pair is composed of only one part:\n","\n","\"\\[ '[CLS]', 'Which', 'year', 'did', 'the', 'USSR', 'cancel', 'the', 'N', '##1', 'rocket', 'program', 'after', 'two', 'failures', 'that', 'didn', \"'\", 't', 'launch', '?', '[SEP]', 'Meanwhile', ',', 'the', 'USSR', 'continued', 'briefly', 'trying', 'to', 'perfect', 'their', 'N', '##1', 'rocket', ',', 'finally', 'cancel', '##ing', 'it', 'in', '1976', ',', 'after', 'two', 'more', 'launch', 'failures', 'in', '1971', 'and', '1972', '.', '[SEP]', '[PAD]', '[PAD]', ..., '[PAD]']\"\n","_______\n","(1) In this example, the answer is fully inside the context, therefore the label should be (start_position, end_position) which here is (41,41) (starting from (0,0) for the [CLS] token). Please note that (41, 41) is the position of the **sub-word token** \"1976\" in the tokenized question+context pair, while 97 here is the index of the first character, \"1\", in the original context (not the question+context pair). However, we expect you to construct the labels based on the sub-word tokens. For your information, this example is the element at index 12245 of the non-tokenized training split of the dataset.\n","\n","(2) In this example, with a different question such as \"When did the two launch failures occured ?\" and a maximum length before truncation smaller, we could have had the \"1972\" sub-word token out of this part of the tokenized question+context pair, therefore, for this part, the label would have been (0,0) as instructed before.\n","\n",">**Note:** If the **max_length** and **stride** parameter of the tokenizer were smaller, then the tokenized context could be truncated in several fragments. This would have allowed the possibility to have the answer not fully included in some tokenized context fragments. In such a case, the labeling of some fragments would be (0,0).\n","\n","> **Important Remark**: Up to now, you've probably used up to 11Gb of RAM (the part of the computer's memory that allows you to retain and quickly access variables during code execution), which is just about the maximum possible for Colab.\n","In the following questions, we'll give you some snippets to help you reduce this memory usage. To prevent your Colab from crashing, we advise you to restart your Colab session and not to run the cells you've coded which process the whole dataset at once. The snippets provided will enable you to use the function you're about to build on the whole dataset, but generatively, i.e. part by part, without loading the whole dataset into memory at once. From now on, you may restart the project from here, without running the first parts (except for the imports and the dataset loading).\n"]},{"cell_type":"markdown","metadata":{"id":"LQ1Q5hsHYNcF"},"source":["### <span style=\"color:#ff8800\">Question 1: Function for labeling</span>\n","\n","Provide here a function named **labeling_dataset** that takes as argument **dataset**, a *Dataset* type from huggingface, that contains as features ['id', 'title', 'context', 'question', 'answers'] and has not been tokenized yet. The function must return the dataset tokenized and labelized. In addition to the labeling, we ask you to store the ids of the questions+contexts in the dataset\n","(Note: As you can see, each question+context in the dataset has a unique id which is one of the feature)\n","\n","This returned dataset must contain those keys:\n","\n","* 'input_ids': the input_ids returned by the tokenizer\n","* 'attention_mask': the attention_mask returned by the tokenizer\n","* 'offset_mapping': a modified version of the offset_mapping returned by the tokenizer reduced to the context part of it\n","* 'question_context_ids': the ids of the correponding questions+contexts\n","* 'start_positions': the first parts of the labels used for training\n","* 'end_positions': the second parts of the labels used for training\n","* 'context_th': the original context (for theoretical context later)\n","* 'answers_th': the original answers (for theoretical answers later)\n","\n","Do not forget that the mappings you got from the tokenizer are essential here.\n","\n","For this question, we give you the function you should complete to be able to answer the following questions. In this function, a comment TODO with the explanation of what you should do is inserted in the code each time you should do something, and \"...\" is inserted where you should code. Be careful not to remove anything, just to complete the code.\n","\n","> **Important Remark**: To find out whether a label is in context, we need to be able to determine where the context begins and ends in the tokenized pair of question+context. To do this, it is possible to detect where the two tokens are [SEP] and say that the context is between them. But again, instead of doing this manually, it's also possible, via the tokenizer output, to determine this automatically. To help doing so, look at the [sequence_ids()][1] function which can only be applied on the output of a tokenizer. In the slides of the course, this method is refered as segment embedding and this function codes the segment embedding as this: the token is 0 if it is in the question and 1 if it is in the context.\n","\n","> **Note:** The tokenizer is not an argument of the function. We ask you to define it globally in your code. Also note that the arguments from the tokenizer should be exactly the same as in the last question.\n","\n","[1]:<https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.sequence_ids>"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"zgsQjsV2r7fe"},"outputs":[],"source":["\n","max_length = 384\n","stride = 128\n","\n","def labeling_dataset(dataset):\n","\n","    # TODO: Extract questions from the dataset\n","    questions = dataset[\"question\"]\n","\n","    # TODO: Extract contexts from the dataset\n","    contexts = dataset[\"context\"]\n","\n","    # TODO: Tokenize the questions and contexts using the tokenizer\n","    tokenized_dataset = tokenizer(\n","       questions, contexts,stride=stride, max_length=max_length, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True\n","    )\n","\n","    # TODO: Get the offset mapping contained in the tokenized_dataset\n","    offset_mapping = tokenized_dataset[\"offset_mapping\"]\n","\n","    # TODO: Extract a sample map defining the correspondence between a\n","    # truncated question+context pair and the id of the original\n","    # question+context pair (i.e. the original dataset before truncation)\n","    sample_map = tokenized_dataset.pop(\"overflow_to_sample_mapping\")\n","\n","    # Get answers from the examples\n","    answers = dataset[\"answers\"]\n","\n","    # Initialize lists to store start and end positions of answers\n","    start_positions = []\n","    end_positions = []\n","\n","    # Initialize a list to store the context IDs\n","    question_context_ids = []\n","\n","    # Initialize lists to store the contexts and answers to build the theoretical answers later\n","    contexts_th = []\n","    answers_th = []\n","\n","    # Loop through the offset mappings\n","    for i, offset in enumerate(offset_mapping):\n","        # TODO: Get the index of the original sample associated with the curent\n","        # iteration of the loop\n","        sample_idx = sample_map[i]\n","\n","        # Get the id of the context\n","        question_context_ids.append(dataset[\"id\"][sample_idx])\n","\n","        # TODO: Get the answer for the current sample\n","        answer = answers[sample_idx]\n","        \n","        # Save the answer and context for the theoretical answer\n","        contexts_th.append(contexts[sample_idx])\n","        answers_th.append(answer)\n","\n","        # TODO: Get the start and end character positions of the answer\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = start_char + len(answer['text'][0])\n","    \n","        # Get the sequence IDs for the current input\n","        # TODO: !!!Please, read the doc of sequence_ids to understand what it does!!!\n","        sequence_ids = tokenized_dataset.sequence_ids(i)\n","\n","        # TODO: Find the start and end positions of the context in the sequence\n","        context_start = sequence_ids.index(1)\n","        context_end   = sequence_ids.index(None, context_start) - 1 \n","        \n","        # TODO: If the answer is outside the context, append 0s to start_positions and end_positions\n","        # Hint: Use the offset to find the context positions.\n","        if start_char < offset[context_start][0] or end_char > offset[context_end][1]:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # TODO: Find the start position (as a token index) of the answer within this context fragment\n","            index_start = context_start\n","            while index_start <= context_end and offset[index_start][0] <= start_char:\n","                index_start += 1\n","            start_positions.append(index_start - 1)\n","\n","            # TODO: Find the end position (as a token index) of the answer within this context fragment\n","            index_end = context_end\n","            while index_end >= context_start and offset[index_end][1] >= end_char:\n","                index_end -= 1\n","            end_positions.append(index_end + 1)\n","\n","        # Only keep offsets that corresponds to the context, remove those from the questions\n","        # For validation performance computation later, one will only look for the answer\n","        # somewhere in the context, not from the question or [CLS], [SEP] or [PAD]\n","        tokenized_dataset[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","       \n","\n","    # Add start_positions and end_positions to the tokenized_dataset dictionary\n","    tokenized_dataset[\"start_positions\"] = start_positions\n","    tokenized_dataset[\"end_positions\"] = end_positions\n","    tokenized_dataset[\"question_context_id\"] = question_context_ids\n","    tokenized_dataset[\"contexts_th\"] = contexts_th\n","    tokenized_dataset[\"answers_th\"] = answers_th\n","\n","    # Return the modified tokenized_dataset\n","    return tokenized_dataset\n"]},{"cell_type":"markdown","metadata":{"id":"Q2jPxMvkT5EC"},"source":["Once your **labeling_dataset** function is done, you should be able to apply it on each elements of both datasets using the snippets below."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"NhO0UGtAT5ED"},"outputs":[],"source":["\n","train_dataset = train_dataset.map(\n","    labeling_dataset,\n","    batched=True,\n","    remove_columns=train_dataset.column_names,\n",")\n","\n","validation_dataset = validation_dataset.map(\n","    labeling_dataset,\n","    batched=True,\n","    remove_columns=validation_dataset.column_names,\n",")\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["88463"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"Me9jV5X7YNcE"},"source":["### <span style=\"color:#ff8800\">Question 2: Labels (1)</span>\n","\n","To test the labelization of the training data, we will ask you to generate several labels in the following questions.\n","\n","Replace the pair of values below by the label for the question+context at index 4242 of the **tokenized** training dataset.\n","\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["247\n","250\n"]}],"source":["\n","print(train_dataset[4242][\"start_positions\"])\n","print(train_dataset[4242][\"end_positions\"])"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"7nxEI_tfYNcE"},"outputs":[{"data":{"text/plain":["(247, 250)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["(247,250)"]},{"cell_type":"markdown","metadata":{"id":"4OkzG6p3YNcE"},"source":["### <span style=\"color:#ff8800\">Question 3: Labels (2)</span>\n","\n","Replace the pair of values below by the label for the question+context at index 1203 of the **tokenized** training dataset.\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n","19\n"]}],"source":["train_dataset[1203]\n","print(train_dataset[1203][\"start_positions\"])\n","print(train_dataset[1203][\"end_positions\"])"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"uZnl49D8YNcF"},"outputs":[{"data":{"text/plain":["(16, 19)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["(16,19)"]},{"cell_type":"markdown","metadata":{"id":"3otacaKLYNcF"},"source":["### <span style=\"color:#ff8800\">Question 4: Labels (3)</span>\n","\n","Replace the pair of values below by the label for the question+context at index 1001 of the **tokenized** training dataset.\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["19\n","20\n"]}],"source":["train_dataset[1001]\n","print(train_dataset[1001][\"start_positions\"])\n","print(train_dataset[1001][\"end_positions\"])"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"rbiLOIdvYNcF"},"outputs":[{"data":{"text/plain":["(19, 20)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["(19,20)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["80\n","82\n"]},{"data":{"text/plain":["{'input_ids': [101,\n","  107,\n","  1284,\n","  4835,\n","  1106,\n","  1301,\n","  1106,\n","  1103,\n","  5148,\n","  107,\n","  4055,\n","  1108,\n","  1549,\n","  1120,\n","  1184,\n","  2450,\n","  1107,\n","  2245,\n","  136,\n","  102,\n","  1124,\n","  16489,\n","  1103,\n","  1788,\n","  1107,\n","  2538,\n","  1104,\n","  1157,\n","  4495,\n","  1106,\n","  1569,\n","  2699,\n","  117,\n","  1105,\n","  1157,\n","  2817,\n","  1104,\n","  1103,\n","  3790,\n","  112,\n","  188,\n","  21963,\n","  1113,\n","  1168,\n","  3812,\n","  1105,\n","  1934,\n","  3872,\n","  119,\n","  1124,\n","  27429,\n","  1927,\n","  1619,\n","  1111,\n","  1103,\n","  1788,\n","  1107,\n","  1117,\n","  107,\n","  1284,\n","  4835,\n","  1106,\n","  1301,\n","  1106,\n","  1103,\n","  5148,\n","  107,\n","  4055,\n","  117,\n","  1113,\n","  1347,\n","  1367,\n","  117,\n","  2832,\n","  117,\n","  1196,\n","  170,\n","  1415,\n","  3515,\n","  1120,\n","  8859,\n","  1239,\n","  3339,\n","  117,\n","  1107,\n","  4666,\n","  117,\n","  2245,\n","  117,\n","  1485,\n","  1103,\n","  2058,\n","  1751,\n","  1104,\n","  1103,\n","  1207,\n","  10852,\n","  1174,\n","  4525,\n","  8444,\n","  1945,\n","  3695,\n","  119,\n","  8896,\n","  3087,\n","  102,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 'attention_mask': [1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 'offset_mapping': [None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  [0, 2],\n","  [3, 12],\n","  [13, 16],\n","  [17, 24],\n","  [25, 27],\n","  [28, 33],\n","  [34, 36],\n","  [37, 40],\n","  [41, 51],\n","  [52, 54],\n","  [55, 63],\n","  [64, 72],\n","  [72, 73],\n","  [74, 77],\n","  [78, 81],\n","  [82, 87],\n","  [88, 90],\n","  [91, 94],\n","  [95, 101],\n","  [101, 102],\n","  [102, 103],\n","  [104, 112],\n","  [113, 115],\n","  [116, 121],\n","  [122, 132],\n","  [133, 136],\n","  [137, 143],\n","  [144, 150],\n","  [150, 151],\n","  [152, 154],\n","  [155, 162],\n","  [163, 170],\n","  [171, 178],\n","  [179, 182],\n","  [183, 186],\n","  [187, 194],\n","  [195, 197],\n","  [198, 201],\n","  [202, 203],\n","  [203, 205],\n","  [206, 212],\n","  [213, 215],\n","  [216, 218],\n","  [219, 221],\n","  [222, 225],\n","  [226, 230],\n","  [230, 231],\n","  [232, 238],\n","  [238, 239],\n","  [240, 242],\n","  [243, 252],\n","  [253, 255],\n","  [255, 256],\n","  [257, 261],\n","  [261, 262],\n","  [263, 269],\n","  [270, 271],\n","  [272, 277],\n","  [278, 283],\n","  [284, 286],\n","  [287, 291],\n","  [292, 302],\n","  [303, 310],\n","  [310, 311],\n","  [312, 314],\n","  [315, 322],\n","  [322, 323],\n","  [324, 329],\n","  [329, 330],\n","  [331, 335],\n","  [336, 339],\n","  [340, 352],\n","  [353, 357],\n","  [358, 360],\n","  [361, 364],\n","  [365, 368],\n","  [369, 373],\n","  [373, 375],\n","  [376, 381],\n","  [381, 386],\n","  [387, 393],\n","  [394, 402],\n","  [402, 403],\n","  [404, 408],\n","  [409, 413],\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None,\n","  None],\n"," 'start_positions': 80,\n"," 'end_positions': 82,\n"," 'question_context_id': '56e0f3a5231d4119001ac4d2',\n"," 'contexts_th': 'He justified the program in terms of its importance to national security, and its focus of the nation\\'s energies on other scientific and social fields. He rallied popular support for the program in his \"We choose to go to the Moon\" speech, on September 12, 1962, before a large crowd at Rice University Stadium, in Houston, Texas, near the construction site of the new Manned Spacecraft Center facility. Full text ',\n"," 'answers_th': {'answer_start': [287], 'text': ['Rice University Stadium']}}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["\n","print(train_dataset[12245][\"start_positions\"])\n","print(train_dataset[12245][\"end_positions\"])\n","train_dataset[12245]"]},{"cell_type":"markdown","metadata":{"id":"xQujRYvJYNcF"},"source":["## <span style=\"color:#ff5500\">**Part Four**: Compute metrics</span>\n","\n","Here is a function called compute_metrics.\n","This function is supposed to take the logits (the values given by the model for each token corresponding to it starting or ending the answer) of the model and to keep the $n$ bests in term of start, and the $n$ bests in term of end. Here, $n$ must be set to **20**\n","\n","After that, for each combination of start and end indexes in those $n$ bests, the function aims at removing answers that wouldn't be inside the context, that have negative length, or that are too long (the limit for an answer should be set to **30** here) and it stores each answer text and logit_score (start_score+end_score). Once done, it keeps only the best answer in terms of logit_score.\n","\n","\n","This function takes as arguments:\n","\n","* **start_logits**: a list of lists containing the encoded values corresponding to the plausibility for tokens to start the answer for each element of the dataset\n","* **end_logits**: a list of lists containing the encoded values corresponding to the plausibility for tokens to end the answer for each element of the dataset\n","* **processed_validation**: the output of the mapping of the function labeling_dataset on the whole validation dataset\n","\n","You are asked to complete this function in order to make it work. The function must return the metrics. You should only replace the \"**...**\" in the function (The replacement can be longer than one line). Help yourself with the comments that are already present in the code.\n","\n","On Inginious, your answer should only contain the function. Not the imports, not the global variables nor the loading of the metric for the dataset."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"q0AwwDIqYNcF"},"outputs":[],"source":["import evaluate\n","import collections\n","import numpy as np\n","\n","metric = evaluate.load(\"squad\")\n","n_best = 20\n","ans_max_len = 30 # We will only accept predicted answer which are not long\n","# (we do not want the model to predict the full context as an answer...)\n","\n","def compute_metrics(start_logits, end_logits, processed_validation):\n","\n","    # Create a dictionnary of lists to map the dataset_ids to the corresponding\n","    # indices of the features\n","    unprocessed_validation_to_processed_validation = collections.defaultdict(list)\n","    for idx, elem in enumerate(processed_validation):\n","        unprocessed_validation_to_processed_validation[elem[\"question_context_id\"]].append(idx)\n","\n","    # To avoid the duplicates due to the truncation of the contexts\n","    seen_ids = set()\n","\n","    # Loop through the dataset\n","    predicted_answers = []\n","    for data in processed_validation:\n","        data_id = data[\"question_context_id\"]\n","\n","        # Avoid considering the duplicates due to the truncation of the context\n","        if data_id in seen_ids:\n","            continue\n","        seen_ids.add(data_id)\n","\n","        context = data[\"contexts_th\"]\n","        answers = []\n","\n","        # Loop through all features associated with that example\n","        for elem_index in unprocessed_validation_to_processed_validation[data_id]:\n","            #print(data_id)\n","            start_logit = start_logits[elem_index]\n","            end_logit = end_logits[elem_index]\n","\n","            # Get the offsets from the labeled dataset\n","            offsets = processed_validation[elem_index][\"offset_mapping\"]\n","\n","            # TODO: get the list of indices of the n_best scores in start_logit and end_logit\n","            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","            #print(f\"Entering the loop of indexes\")\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    if end_index >= start_index:\n","                        \n","                        # TODO: Skip predicted answers that are not fully in the context\n","                        # (a quick look at last lines of the for loop of labeling_dataset could help doing this)\n","                        if offsets[start_index] is None or offsets[end_index] is None:\n","                            continue\n","                        # TODO: Skip predicted answers with a length that is > ans_max_len\n","                        if (end_index < start_index\n","                            or end_index - start_index + 1 > ans_max_len):\n","                            continue\n","\n","                        # TODO: \"text\" must be the text corresponding with the answer\n","                        # and \"logit_score\" must be the sum of both start and end logit scores\n","                        answer = {\n","                            \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                        }\n","                        #print(f\"answer add to answers {answer}\")\n","                        answers.append(answer)\n","        \n","        # TODO: Select the answer with the best score and handle case where there is no answer in answers\n","        # by returning an empty answer.\n","\n","        if len(answers) > 0:\n","            #print(answers)\n","            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","            #print(best_answer)\n","            predicted_answers.append({\"id\": data_id, \"prediction_text\": best_answer[\"text\"]})\n","            \n","        else:\n","            predicted_answers.append({\"id\": data_id, \"prediction_text\": []})\n","    print(f\"len seen ids set {len(seen_ids)}\")\n","    print(f\"End the loop of indexes\")\n","    # TODO: Get the corresponding theoretical answers\n","    theoretical_answers = []#{\"id\": ex[\"question_context_id\"], \"answers\": ex[\"answers_th\"]} for ex in processed_validation\n","    #ans = processed_validation[\"answers_th\"]\n","    #ids = processed_validation[\"question_context_id\"]\n","    seen_ids = set()    # Avoid considering the duplicates\n","    print(\"just before loop answer\")\n","    print(f\"len answers {len(processed_validation['answers_th'])}\")\n","\n","    for ex in processed_validation:\n","        if ex[\"question_context_id\"] not in seen_ids:\n","            theoretical_answers.append({\"id\": ex[\"question_context_id\"], \"answers\": ex[\"answers_th\"]} )\n","            seen_ids.add(ex[\"question_context_id\"])\n"," \n","    print(\"end of answers loop\")\n","    #print(f\"prediction = {predicted_answers}\")\n","    #print(f\"references = {theoretical_answers}\")\n","    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"]},{"cell_type":"markdown","metadata":{"id":"HPbnZ_MnYNcG"},"source":["## <span style=\"color:#ff5500\">**Part Five**: Training and validation loop</span>\n","\n","In this part, you are asked to complete the code below (i.e. replace the '...' in the code with your own code, with a replacement that can be longer than one line). This code should allow to train your model on the training split of the dataset using batches of data (combination of consecutives elements of the dataset into groups) and validate its performance on the validation split, also using batches of data. This training and validation by batches avoid loading too much data into memory at the same time as only a fraction of the data is used at each step of the training instead of the whole dataset.\n","\n","The code below is separated into 2 different sections.\n","\n",">**Note**: A part of both code sections uses the python library called **accelerate**. This is needed to speedup the code enough to be usable on the Colab calculation clusters. It uses different techniques that are not described in this course but for those interested, here is the [documentation][1] of the library.\n","\n","[1]: <https://huggingface.co/docs/accelerate/index>"]},{"cell_type":"markdown","metadata":{"id":"6QmN4ybKYNcG"},"source":["### <span style=\"color:#ff8800\">**Section One**: Dataloaders and hyperparameters</span>\n","\n","In first section, we setup the [model][1], the [dataloaders][2], the [AdamW optimizer][3] for the gradient descent and finally the [scheduler][4] to manage the learning during the training process. In particular, we remove two columns from each dataloader -**question_context_id** and **offset_mapping**- as the [model][1] **does not allow** them in its [**forward()**][5] function.\n","\n","You should fill the blanks with the conditions we give you here:\n","\n","* The batch sizes for both training and validation dataloaders should be $8$\n","* The total number of training epochs must be set regarding the following questions\n","* The training dataset should be **shuffled**\n","* The learning rate of the optimizer should be $2\\times 10^{-5}$\n","* The scheduler should be **linear**\n","* The scheduler should have $0$ warmup steps\n","* The number of training steps of the scheduler should be (the number of training epochs) $\\times$ (the number of elements in the training dataset)\n","\n","Nothing should be done that is not given **explicitly** in those conditions !\n","\n","[1]: <https://huggingface.co/transformers/v3.0.2/model_doc/distilbert.html#transformers.DistilBertForQuestionAnswering>\n","[2]: <https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>\n","[3]: <https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html>\n","[4]: <https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_scheduler>\n","[5]: <https://huggingface.co/transformers/v3.0.2/model_doc/distilbert.html#transformers.DistilBertForQuestionAnswering.forward>"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"0srUs3juYNcG"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# SECTION 1: Dataloaders and hyperparameters\n","\n","### Imports\n","\n","from torch.utils.data import DataLoader\n","from transformers import get_scheduler, default_data_collator, AutoModelForQuestionAnswering\n","from torch.optim import AdamW\n","from accelerate import Accelerator\n","\n","\n","### Dataloaders\n","\n","# Remove columns that are not allowed by the model\n","\n","# Note: train_dataset and validation_dataset are of type DatasetDict and\n","# correspond to the train and validation splits of the squad_dataset\n","train_set = train_dataset.remove_columns([\"question_context_id\",\"offset_mapping\", \"contexts_th\", \"answers_th\"])\n","train_set.set_format(\"torch\")\n","validation_set = validation_dataset.remove_columns([\"question_context_id\",\"offset_mapping\", \"contexts_th\", \"answers_th\"])\n","validation_set.set_format(\"torch\")\n","\n","# TODO: complete to meet conditions\n","train_dataloader = DataLoader(\n","    train_set,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n","    shuffle=True,\n",")\n","# TODO: complete to meet conditions\n","val_dataloader = DataLoader(\n","    validation_set,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n",")\n","\n","\n","### Model and optimizer\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","# TODO: complete to meet conditions\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","\n","#############################################\n","#############accelerate part#################\n","accelerator = Accelerator(mixed_precision='fp16')\n","model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, val_dataloader\n",")\n","##########end of accelerate part#############\n","#############################################\n","\n","\n","### Scheduler\n","\n","# TODO: complete to meet conditions\n","num_train_epochs = 1\n","lr_scheduler = get_scheduler(\n","    name=\"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_train_epochs*len(train_dataloader)\n",")"]},{"cell_type":"markdown","metadata":{"id":"myW6GbrTYNcG"},"source":["###  <span style=\"color:#ff8800\">**Section Two**: Training and validation </span>\n","\n","In this section, the training and validation loop is built. For each epoch, a first loop iterates over each batch of the train dataloader. This is the training loop. Its objective, through backpropagation, is to minimize the loss of the model.\n","\n","**Training loop:**\n","\n","(1) You get the outputs of the model, and the loss stored in it.\\\n","(2) Using it, the backpropagation of the gradient is done. \\\n","(3) After that, you should update the parameters (i.e. the weights and biases).\\\n","(4) Then, you should update the learning rate.\\\n","(5) Finally, you should reset the gradients to prevent an accumulation.\n","\n","Once the training loop has been perform, the model mode changes to evaluation (= validation) to verify if the model learns something through epochs. This validation loop iterates over each batch of the validation dataloader.\n","\n","**Validation loop:**\n","\n","(1) Each batch passes through the model, and the outputs are collected.\\\n","(2) From those outputs, the start and end logits are extracted on the CPU in the form of [numpy][1] arrays and appended to lists of start and end logits.  \n","\n","Once both lists are completed, the **compute_metrics** you created earlier should be able to compute both **exact-match** score and **f1-score** for this epoch. Note that the last line of the function **compute_metrics** computes automatically both the **exact-match** and **f1-score**. For those interested, here are some ressources to better understand both metrics:\n","\n","* https://en.wikipedia.org/wiki/F-score\n","* https://www.v7labs.com/blog/f1-score-guide\n","* https://torchmetrics.readthedocs.io/en/stable/classification/exact_match.html\n","* https://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/qa/\n","\n","[1]: <https://numpy.org/doc/stable/reference/generated/numpy.array.html>"]},{"cell_type":"markdown","metadata":{"id":"VVFlTx_AC9wq"},"source":["> **NOTE**: Do **NOT** run the following cell without **rerunning** all the dataset processing and the creation of the model. If you don't, it will be like if you train your model for one more epoch."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"U68DuHL0YNcH"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/11058 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Entering training process\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 11058/11058 [42:03<00:00,  4.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Entering evaluation process\n","loop batch end\n","len seen ids set 10536\n","End the loop of indexes\n","just before loop answer\n","len answers 10784\n","end of answers loop\n","epoch 0: {'exact_match': 74.51594533029613, 'f1': 83.14374759640418}\n"]}],"source":["# (Optional, if you want a progress bar)\n","from tqdm.auto import tqdm\n","progress_bar = tqdm(range(num_train_epochs*len(train_dataloader)))\n","import torch\n","\n","\n","for epoch in range(num_train_epochs):\n","    # Training\n","    print(f\"Entering training process\")\n","    \n","    # This instruction sets the model to training mode (as opposed to evaluation mode)\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Get ouputs and loss from the model\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","\n","        # Backpropagation\n","        #############################################\n","        #############accelerate part#################\n","        accelerator.backward(loss)\n","        #############end of accelerate part##########\n","        #############################################\n","\n","        # TODO: Update model parameters (i.e. weights and biases of the model) by performing an optimization step\n","        # Hint: look at optimizer\n","        optimizer.step()\n","\n","        # TODO: Update learning rate\n","        # Hint: look at lr_scheduler\n","        lr_scheduler.step()\n","\n","        # TODO: Zero gradients to prevent gradient accumulation\n","        optimizer.zero_grad()\n","\n","        # (Optional, if you want a progress bar)\n","        progress_bar.update(1)\n","\n","    # Evaluation\n","    \n","    # This instruction sets the model to evaluation mode (as opposed to training mode)\n","    print(f\"Entering evaluation process\")\n","    model.eval()\n","\n","    # This initializes lists to store the predicted logits for start and end positions\n","    start_logits = []\n","    end_logits = []\n","\n","\n","    for i,batch in enumerate(val_dataloader):\n","        #if i % 50 == 0:\n","            #print(f\"iteration of the batch number {i}/{len(val_dataloader)}\")\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n","        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n","    print(\"loop batch end\")\n","    start_logits = np.concatenate(start_logits)\n","    end_logits = np.concatenate(end_logits)\n","    start_logits = start_logits[: len(validation_dataset)]\n","    end_logits = end_logits[: len(validation_dataset)]\n","\n","    metrics = compute_metrics(\n","        start_logits, end_logits, validation_dataset\n","    )\n","    print(f\"epoch {epoch}:\", metrics)\n","\n","    # (Optional) If you want to save your model:\n","    #accelerator.wait_for_everyone()\n","    #unwrapped_model = accelerator.unwrap_model(model)\n","    #unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n","    # if accelerator.is_main_process:\n","    #     tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["max_length = 384\n","stride = 128\n","\n","def labeling_dataset(dataset):\n","\n","    # TODO: Extract questions from the dataset\n","    questions = dataset[\"question\"]\n","\n","    # TODO: Extract contexts from the dataset\n","    contexts = dataset[\"context\"]\n","\n","    # TODO: Tokenize the questions and contexts using the tokenizer\n","    tokenized_dataset = tokenizer(\n","       questions, contexts,stride=stride, max_length=max_length, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True\n","    )\n","\n","    # TODO: Get the offset mapping contained in the tokenized_dataset\n","    offset_mapping = tokenized_dataset[\"offset_mapping\"]\n","\n","    # TODO: Extract a sample map defining the correspondence between a\n","    # truncated question+context pair and the id of the original\n","    # question+context pair (i.e. the original dataset before truncation)\n","    sample_map = tokenized_dataset.pop(\"overflow_to_sample_mapping\")\n","\n","    # Get answers from the examples\n","    answers = dataset[\"answers\"]\n","\n","    # Initialize lists to store start and end positions of answers\n","    start_positions = []\n","    end_positions = []\n","\n","    # Initialize a list to store the context IDs\n","    question_context_ids = []\n","\n","    # Initialize lists to store the contexts and answers to build the theoretical answers later\n","    contexts_th = []\n","    answers_th = []\n","\n","    # Loop through the offset mappings\n","    for i, offset in enumerate(offset_mapping):\n","        # TODO: Get the index of the original sample associated with the curent\n","        # iteration of the loop\n","        sample_idx = sample_map[i]\n","\n","        # Get the id of the context\n","        question_context_ids.append(dataset[\"id\"][sample_idx])\n","\n","        # TODO: Get the answer for the current sample\n","        answer = answers[sample_idx]\n","        \n","        # Save the answer and context for the theoretical answer\n","        contexts_th.append(contexts[sample_idx])\n","        answers_th.append(answer)\n","\n","        # TODO: Get the start and end character positions of the answer\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = start_char + len(answer['text'][0])\n","    \n","        # Get the sequence IDs for the current input\n","        # TODO: !!!Please, read the doc of sequence_ids to understand what it does!!!\n","        sequence_ids = tokenized_dataset.sequence_ids(i)\n","\n","        # TODO: Find the start and end positions of the context in the sequence\n","        context_start = sequence_ids.index(1)\n","        context_end   = sequence_ids.index(None, context_start) - 1 \n","        \n","        # TODO: If the answer is outside the context, append 0s to start_positions and end_positions\n","        # Hint: Use the offset to find the context positions.\n","        if start_char < offset[context_start][0] or end_char > offset[context_end][1]:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # TODO: Find the start position (as a token index) of the answer within this context fragment\n","            index_start = context_start\n","            while index_start <= context_end and offset[index_start][0] <= start_char:\n","                index_start += 1\n","            start_positions.append(index_start - 1)\n","\n","            # TODO: Find the end position (as a token index) of the answer within this context fragment\n","            index_end = context_end\n","            while index_end >= context_start and offset[index_end][1] >= end_char:\n","                index_end -= 1\n","            end_positions.append(index_end + 1)\n","\n","        # Only keep offsets that corresponds to the context, remove those from the questions\n","        # For validation performance computation later, one will only look for the answer\n","        # somewhere in the context, not from the question or [CLS], [SEP] or [PAD]\n","        tokenized_dataset[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","       \n","\n","    # Add start_positions and end_positions to the tokenized_dataset dictionary\n","    tokenized_dataset[\"start_positions\"] = start_positions\n","    tokenized_dataset[\"end_positions\"] = end_positions\n","    tokenized_dataset[\"question_context_id\"] = question_context_ids\n","    tokenized_dataset[\"contexts_th\"] = contexts_th\n","    tokenized_dataset[\"answers_th\"] = answers_th\n","\n","    # Return the modified tokenized_dataset\n","    return tokenized_dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","squad_dataset = load_dataset(\"squad\")\n","import csv\n","path = \"indices.csv\"#/content/drive/MyDrive/\n","indices_to_remove_train = []\n","indices_to_remove_valid = []\n","with open(path) as f:\n","    reader_obj = csv.reader(f)\n","    indices_to_remove_train = list(map(int, next(reader_obj)))\n","    indices_to_remove_valid = list(map(int, next(reader_obj)))\n","\n","train_dataset = squad_dataset[\"train\"]\n","train_dataset = train_dataset.filter(lambda example, idx: idx not in indices_to_remove_train, with_indices=True)\n","\n","validation_dataset = squad_dataset[\"validation\"]\n","validation_dataset = validation_dataset.filter(lambda example, idx: idx not in indices_to_remove_valid, with_indices=True)\n","\n","from transformers import AutoTokenizer\n","\n","model_checkpoint = \"distilbert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","\n","train_dataset = train_dataset.map(\n","    labeling_dataset,\n","    batched=True,\n","    remove_columns=train_dataset.column_names,\n",")\n","\n","validation_dataset = validation_dataset.map(\n","    labeling_dataset,\n","    batched=True,\n","    remove_columns=validation_dataset.column_names,\n",")\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# SECTION 1: Dataloaders and hyperparameters\n","\n","### Imports\n","\n","from torch.utils.data import DataLoader\n","from transformers import get_scheduler, default_data_collator, AutoModelForQuestionAnswering\n","from torch.optim import AdamW\n","from accelerate import Accelerator\n","\n","\n","### Dataloaders\n","\n","# Remove columns that are not allowed by the model\n","\n","# Note: train_dataset and validation_dataset are of type DatasetDict and\n","# correspond to the train and validation splits of the squad_dataset\n","train_set = train_dataset.remove_columns([\"question_context_id\",\"offset_mapping\", \"contexts_th\", \"answers_th\"])\n","train_set.set_format(\"torch\")\n","validation_set = validation_dataset.remove_columns([\"question_context_id\",\"offset_mapping\", \"contexts_th\", \"answers_th\"])\n","validation_set.set_format(\"torch\")\n","\n","# TODO: complete to meet conditions\n","train_dataloader = DataLoader(\n","    train_set,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n","    shuffle=True,\n",")\n","# TODO: complete to meet conditions\n","val_dataloader = DataLoader(\n","    validation_set,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n",")\n","\n","\n","### Model and optimizer\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","# TODO: complete to meet conditions\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","\n","#############################################\n","#############accelerate part#################\n","accelerator = Accelerator(mixed_precision='fp16')\n","model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, val_dataloader\n",")\n","##########end of accelerate part#############\n","#############################################\n","\n","\n","### Scheduler\n","\n","# TODO: complete to meet conditions\n","num_train_epochs = 1\n","lr_scheduler = get_scheduler(\n","    name=\"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_train_epochs*len(train_dataloader)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seen_ids = set()\n","theoretical_answers = []\n","for ex in validation_dataset:\n","    if ex[\"question_context_id\"] not in seen_ids:\n","        theoretical_answers.append({\"id\": ex[\"question_context_id\"], \"answers\": ex[\"answers_th\"]} )\n","        seen_ids.add(ex[\"question_context_id\"])\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theoretical_answers[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(theoretical_answers))\n","print(len(seen_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_ids_in_dataset = set(ex[\"question_context_id\"] for ex in validation_dataset)\n","print(\"Number of unique IDs in dataset:\", len(unique_ids_in_dataset))"]},{"cell_type":"markdown","metadata":{"id":"2GrsbYUWYNcH"},"source":["### <span style=\"color:#ff8800\">Question 1: No training\n","\n","Report the exact match and the F1-score on the validation set **without any training** of the model (you could comment the part of your code between **model.train()** (included) and **model.eval()** (excluded) or split these two parts in functions for genericity). To be sure you have the same results as us, we ask you to put the instructions **torch.manual_seed(2809)** in the exact place mentionned in the instruction below, as we want you all to have the same result. Please note that this instruction should be removed for all other question of this part of the project.\n","\n","```python\n","torch.manual_seed(2809)\n","# TODO: complete to meet conditions\n","val_dataloader = DataLoader(\n","    validation_set,\n","    collate_fn=default_data_collator,\n","    ...\n",")\n","```\n","\n","Your answer should follow the format below. The keys should be \"f1-score\" and \"exact_match\" and the values should be the scores you obtained, as floats with **3** decimals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGmTOQ7rT5EO"},"outputs":[],"source":["{\"f1-score\": 0.0, \"exact_match\": 0.0}\n","epoch_0: {'exact_match': 0.17084282460136674, 'f1': 7.324965098531754}"]},{"cell_type":"markdown","metadata":{"id":"N4f3T0fMT5EO"},"source":["### <span style=\"color:#ff8800\">Question 2: One epoch\n","\n","Report the exact match and the F1-score on the validation set **after one epoch** of training of the model (you should comment the part of your code between **model.train()** (included) and **model.eval()** (excluded)).\n","\n","Your answer should follow the format below. The keys should be \"f1-score\" and \"exact_match\" and the values should be the scores you obtained, as floats with **3** decimals.\n","\n","> **Note**: Don't forget to remove the seed you have set for previous question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJsKYaE8T5EO"},"outputs":[],"source":["{\"f1-score\": 74.41154138192863, 'f1': 82.8884153647775}\n","epoch_0: {'exact_match': 73.6617312072893, 'f1': 82.27001871065184}\n","epoch_0: {'exact_match': 74.2786636294609, 'f1': 82.90870266841351}"]},{"cell_type":"markdown","metadata":{"id":"v6hqTtyZT5EO"},"source":["### <span style=\"color:#ff8800\">Question 3: Three epochs\n","\n","Report the exact match and the F1-score on the validation set **after three epochs** of training of the model.\n","\n","Your answer should follow the format below. The keys should be \"f1-score\" and \"exact_match\" and the values should be the scores you obtained, as floats with **3** decimals.\n","\n","> **Note:** You should probably save this model in order to be able to use on the inference questions (questions 5 and 6)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1W96ptJT5EP"},"outputs":[],"source":["{\"f1-score\": 0.0, \"exact_match\": 0.0}\n","epoch_2: {'exact_match': 76.58504176157935, 'f1': 84.74914482799304}"]},{"cell_type":"markdown","metadata":{"id":"KgATJjgGT5EP"},"source":["### <span style=\"color:#ff8800\">Question 4: One epoch with change of the max_length and the stride\n","\n","Report the exact match and the F1-score on the validation set **after one epoch** of training of the model. To evaluate the model, refactor the **validation_dataset** changing the value of **max_length** to 256 and **stride** to 85 respectively.\n","\n","Your answer should follow the format below. The keys should be \"f1-score\" and \"exact_match\" and the values should be the scores you obtained, as floats with **3** decimals.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETdedH9bT5EP"},"outputs":[],"source":["{\"f1-score\": 0.0, \"exact_match\": 0.0}\n","epoch_0: {'exact_match': 74.39255884586181, 'f1': 82.99542935881107}\n","epoch_0: {'exact_match': 74.50645406226272, 'f1': 83.22487190591889}"]},{"cell_type":"markdown","metadata":{"id":"avaqkFZET5EP"},"source":["### <span style=\"color:#ff8800\">Question 5: Inference (1)\n","\n","Consider the model trained on three epochs (the result of Question 3) and use it to predict the answer on a novel question+context pair(not in the squad dataset):\n","\n","* **Question:** \"How can I use the fine-tuned SQuAD model to answer a new question given a context?\"\n","* **Context:** \"The SQuAD model is a question-answering model that has been fine-tuned on the Stanford Question Answering Dataset (SQuAD). It can be used to answer new questions given a context. The context is a piece of text that contains the information needed to answer the question. To use the model, you need to pass the question and the context to the model's prediction function. The model will then return the answer to the question based on the information in the context.\"\n","\n","You should not retrain your model on this question+context ! You should tokenize your question+context, then pass them through the model in a forward pass (no backward pass ! cfr. code of section 2 of part five in the notebook) and finally, you should apply a similar process to the one you used in the compute_metrics function.\n","\n","Your answer should be the decoded answer given by the model in the format below:"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","from transformers import pipeline\n","model_checkpoint = \"distilbert-base-cased\"\n","max_length = 384\n","stride = 128\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, stride=stride, max_length=max_length, padding=\"max_length\", return_overflowing_tokens=True, truncation=\"only_second\", return_offsets_mapping=True)\n","train_model = AutoModelForQuestionAnswering.from_pretrained(\"pretrained/\")\n","question1 = \"How can I use the fine-tuned SQuAD model to answer a new question given a context?\"\n","context1  = \"The SQuAD model is a question-answering model that has been fine-tuned on the Stanford Question Answering Dataset (SQuAD). It can be used to answer new questions given a context. The context is a piece of text that contains the information needed to answer the question. To use the model, you need to pass the question and the context to the model's prediction function. The model will then return the answer to the question based on the information in the context.\""]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["question_answerer = pipeline(\"question-answering\", model=\"pretrained/\", tokenizer=tokenizer)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["{'score': 0.1985926628112793,\n"," 'start': 78,\n"," 'end': 113,\n"," 'answer': 'Stanford Question Answering Dataset'}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["question_answerer(question1, context1)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["{'score': 0.9978997707366943,\n"," 'start': 78,\n"," 'end': 105,\n"," 'answer': 'Jax, PyTorch and TensorFlow'}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Replace this with your own checkpoint\n","model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n","question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n","\n","context = \"\"\"\n"," Transformers is backed by the three most popular deep learning libraries  Jax, PyTorch and TensorFlow  with a seamless integration\n","between them. It's straightforward to train your models with one before loading them for inference with the other.\n","\"\"\"\n","question = \"Which deep learning libraries back  Transformers?\"\n","question_answerer(question=question, context=context)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["{'score': 0.059110213071107864,\n"," 'start': 194,\n"," 'end': 269,\n"," 'answer': 'a piece of text that contains the information needed to answer the question'}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["question_answerer(question=question1, context=context1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyO8okjGT5EP"},"outputs":[],"source":["{'score': 0.1985926628112793,\n"," 'start': 78,\n"," 'end': 113,\n"," 'answer': 'Stanford Question Answering Dataset'}"]},{"cell_type":"markdown","metadata":{"id":"HaNCkGOET5EP"},"source":["### <span style=\"color:#ff8800\">Question 6: Inference (2)\n","\n","Consider the model trained on three epochs (the result of Question 3) and use it to predict the answer on a novel question+context pair(not in the squad dataset):\n","\n","* **Question:** \"Who is the author of the book 'To Kill a Mockingbird'?\"\n","* **Context:** \"'To Kill a Mockingbird' is a novel by the American author Harper Lee. Published in 1960, it was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The plot and characters are loosely based on Lee's observations of her family, her neighbors and an event that occurred near her hometown of Monroeville, Alabama, in 1936, when she was 10 years old.\"\n","\n","You should not retrain your model on this question+context ! You should tokenize your question+context, then pass them through the model in a forward pass (no backward pass ! cfr. code of section 2 of part five in the notebook) and finally, you should apply a similar process to the one you used in the compute_metrics function.\n","\n","Your answer should be the decoded answer given by the model in the format below:"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["{'score': 0.9766094088554382, 'start': 58, 'end': 68, 'answer': 'Harper Lee'}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["question2 = \"Who is the author of the book 'To Kill a Mockingbird'?\"\n","context2  = \"'To Kill a Mockingbird' is a novel by the American author Harper Lee. Published in 1960, it was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The plot and characters are loosely based on Lee's observations of her family, her neighbors and an event that occurred near her hometown of Monroeville, Alabama, in 1936, when she was 10 years old.\"\n","question_answerer(question2, context2)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"krnIVxJ1T5EQ"},"outputs":[{"data":{"text/plain":["'This is the answer of the model'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["{'score': 0.9964790344238281, 'start': 58, 'end': 68, 'answer': 'Harper Lee'}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"myvenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
